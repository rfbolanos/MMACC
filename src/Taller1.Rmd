---
title: "MMACC- Análisis Avanzado de Datos:  Taller1"
author: Raúl Andrés Rodriguez - Richard Felipe Bolaños
output: html_document
date: "2024-02-27"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r paquetes_configuracion,include=FALSE, echo=FALSE}

library(pacman)

p_load(tidyverse,kableExtra,glmnet,data.table,car,pracma,knitr,ISLR2)

taller1<-fread("taller1.txt",sep="auto")


```

## Problema

El conjunto de datos taller1.txt contiene la información del perfíl
genómico de un conjunto de 1200 líneas celulares. Para estas se busca
determinar cuáles de los 5000 genes (ubicados en cada columna) son de
relevancia para la predicción de la variable respuesta (efectividad del
tratamiento anticancer, medida como variable continua).

## Dataset

```{r, echo=FALSE, warning=FALSE}

head(taller1) %>% data.frame() %>% select(y,V1,V2,V3,V4,V5,V4995,V4996,V4997,V4998,V4999,V5000) %>% 
knitr::kable(booktabs = FALSE,align="c") %>%
kable_styling(position = "center")


```

## Pregunta \# 1

¿Hay multicolinealidad en los datos? Explique sucintamente

Dado que la matriz de diseño es de dimensión $n\leq p$ usar tecnicas
como el VIF no es posible y calcular las correlaciones entre las
covariables no es necesariamente un sintoma de multicolinealidad, por lo
que la existencia de esta misma puede ocurrir por la relación implicita
que existe por la naturaleza de las variables. Sin embargo como uno de
los principios de la existencia multicolinealidad es que las variables
regresoras no sean ortogonales, es posible que exista al menos una
variable regresora que es producto de una combinación lineal de otras
variables regresoras, en ese sentido la existencia de la
multicolinealidad es latente en el conjunto de datos.

## Pregunta \# 2

Separe aleatoriamente (pero guarde la semilla) su conjunto de datos en
dos partes: Entrenamiento: 1000 líneas celulares Prueba: 200 líneas
celulares.

```{r}
# Establecer la semilla para reproducibilidad
set.seed(123)

# Crear un vector de índices de filas
indices <- 1:1200

# Seleccionar aleatoriamente 1000 índices para el primer conjunto
indices_1000 <- sample(indices, 1000)

# Crear el primer conjunto de datos con 1000 filas
taller1_1000 <- taller1[indices_1000, ]

# Remover los índices seleccionados para el primer conjunto
indices <- setdiff(indices, indices_1000)

# Los índices restantes serán utilizados para el segundo conjunto de datos
taller1_200 <- taller1[indices, ]
```

## Pregunta \# 3

Usando los 1000 datos de entrenamiento, determine los valores de
$\lambda_r$ y $\lambda_l$ de regesión ridge y lasso, respectivamente,
que minimicen el error cuadrático medio (ECM) mediante validación
externa. Utilice el método de validación externa que considere más
apropiado

Usando la validación cruzada en fold como validación externa, se
evaluaran 20 lambdas espaciados en escala logaritmica en 10 folds y
encontrar el mejor lambda para la regresión *Ridge* y *Lasso*.

```{r message=FALSE, warning=FALSE}

lambda<-logseq(0.05, 1, 25)


## RIDGE

ridge_models <- cv.glmnet(x=as.matrix(taller1_1000[, -1]), y=taller1_1000$y,
                          alpha=0, 
                          lambda=lambda,
                          nfolds=10)

## LASSO
lasso_models <- cv.glmnet(x=as.matrix(taller1_1000[, -1]), y=taller1_1000$y,
                          alpha=1, 
                          lambda=lambda,
                          nfolds=10)
```

```{r echo=FALSE, warning=FALSE}

data.frame(Lambdas= c("Min","1se"),
           Ridge=c(ridge_models$lambda.min,ridge_models$lambda.1se),
           Lasso=c(lasso_models$lambda.min,lasso_models$lambda.1se)) %>% 
  knitr::kable(booktabs = FALSE,align="c") %>% kable_styling(position = "center")


```

## Pregunta \# 4

Ajuste la regresión ridge y lasso con los valores estimados de
$\lambda_r$ y $\lambda_l$ obtenidos en (3) usando los 1000 datos de
entrenamiento.

```{r message=FALSE, warning=FALSE}

#RIDGE
ridge_model_min <- glmnet(x=as.matrix(taller1_1000[, -1]), y=taller1_1000$y,
                          alpha=0, lambda=ridge_models$lambda.min)

ridge_model_1se <- glmnet(x=as.matrix(taller1_1000[, -1]), y=taller1_1000$y,
                          alpha=0, lambda=ridge_models$lambda.1se)


#LASSO
lasso_model_min <- glmnet(x=as.matrix(taller1_1000[, -1]), y=taller1_1000$y,
                          alpha=1, lambda=lasso_models$lambda.min)

lasso_model_1se <- glmnet(x=as.matrix(taller1_1000[, -1]), y=taller1_1000$y, 
                          alpha=1, lambda=lasso_models$lambda.1se)
```

## Pregunta \# 5

Para los modelos ajustados en (4) determine el más apropiado para
propósitos de predicción. Considere únicamente el ECM en los 200 datos
de prueba para su decisión.

```{r message=FALSE, warning=FALSE}

#No. 1 
# Realizar predicciones en los datos de prueba
predicciones_rmin <- predict(ridge_model_min, as.matrix(taller1_200[, -1]))

# Calcular el error cuadrático medio
ecm_ridge_min <- mean((predicciones_rmin - taller1_200$y)^2)

#No. 2 
# Realizar predicciones en los datos de prueba
predicciones_rse1 <- predict(ridge_model_1se, as.matrix(taller1_200[, -1]))

# Calcular el error cuadrático medio
ecm_ridge_1se <- mean((predicciones_rse1 - taller1_200$y)^2)

#No. 3 
# Realizar predicciones en los datos de prueba
predicciones_lmin <- predict(lasso_model_min, as.matrix(taller1_200[, -1]))

# Calcular el error cuadrático medio
ecm_lasso_min <- mean((predicciones_lmin - taller1_200$y)^2)

#No. 4 
# Realizar predicciones en los datos de prueba
predicciones_lse1 <- predict(lasso_model_1se, as.matrix(taller1_200[, -1]))

# Calcular el error cuadrático medio
ecm_lasso_1se <- mean((predicciones_lse1 - taller1_200$y)^2)


```

```{r echo=FALSE}

df<-data.frame(Modelos=rep(c("Ridge","Lasso"),each= 2),Lambdas=c("Min","1Se"),
           Value=c(ridge_models$lambda.min,ridge_models$lambda.1se,
                   lasso_models$lambda.min,lasso_models$lambda.1se),
           ECM= c(ecm_ridge_min, ecm_ridge_1se,ecm_lasso_min, ecm_lasso_1se))  

df[4,4]<- cell_spec(df[4,4], bold = T, color = "green" )
 
 knitr::kable(df,format = "html",align="c",escape=FALSE,
              format.args = list(decimal.mark = ".") )%>% kable_styling(position = "center") 
  
```

En ese sentido el modelo que tuvo menor error cuadratico medio es el
modelo lasso con el $\lambda_{min}$, Sin embargo se sugiere utilizar el
$\lambda_{1se}$ por qué penaliza más variables que el $\lambda_{min}$.

## Pregunta \# 6

Ajuste el modelo seleccionado en (5) para los 1200 datos. Note que en
este punto ya tiene un $\lambda$ y un modelo seleccionado.

Se realizan dos modelos uno con el lambda ajustado para fines de la
predicción y otro modelo para realizar el grafico de las trazas.

```{r}

Modelo.fit<-glmnet(y=as.matrix(taller1[,1]),x= as.matrix(taller1[,-1]),alpha = 1,lambda=0.10573713)

Modelo<-glmnet(y=as.matrix(taller1[,1]),x= as.matrix(taller1[,-1]),alpha = 1,lambda=lambda)


```

## Pregunta \# 7

Grafique las trazas de los coeficientes en función de la penalización
para el modelo ajustado en 6

```{r}

plot(Modelo, label = TRUE,xvar = "lambda",
     main="Trazas de los coeficcientes vs Log(Lambdas)")


```

## Pregunta \# 8

En un párrafo resuma los resultados obtenidos dado el objetivo inicial
del estudio.

Dado el conjunto de datos que contiene la información del perfil
genómico de 1200 líneas celulares y buscando determinar cuáles
covariables son de relevancia para la predicción de la variable
respuesta definida como la efectividad del tratamiento anticáncer, se
realizó un ejercicio que en primer lugar determinó la multicolinealidad
del conjunto de datos dado que cuenta con un mayor número de columnas
frente a las observaciones, lo cual se explica detalladamente en el
punto 1. Seguidamente se realizó la validación de modelos de regresión
Ridge y Lasso para determinar el modelo más adecuado utilizando el ECM
como variable de selección y el nivel de penalización.

El resultado corresponde a un modelo de regresión Lasso, el cual a
través del análisis de los coeficientes nos permite determinar las
variables menos relevantes para la predicción las cuales tienen un
coeficiente con valor cero. Este análisis de los coeficientes del modelo
Lasso permite no solo identificar los genes más importantes para la
predicción, sino también reducir el número de variables a considerar en
futuros análisis, lo que simplifica el modelo y posiblemente mejora su
interpretabilidad y generalización, cumpliendo así con el objetivo
propuesto.
